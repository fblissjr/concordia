{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLM Inference with MLX-LM on Apple Silicon\n",
    "\n",
    "This notebook demonstrates how to run Concordia simulations using MLX-LM for\n",
    "local inference on Apple Silicon (M1/M2/M3/M4 Macs).\n",
    "\n",
    "**Requirements:**\n",
    "- macOS with Apple Silicon (M1/M2/M3/M4)\n",
    "- Python 3.12+\n",
    "- MLX and MLX-LM installed\n",
    "\n",
    "**Benefits of local inference:**\n",
    "- No API keys required\n",
    "- No per-token costs\n",
    "- Data stays on your machine\n",
    "- Fast inference on Apple Silicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install dependencies (if needed)\n",
    "# Uncomment and run if you haven't installed the dependencies yet\n",
    "# %pip install gdm-concordia[mlx_lm] sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "from concordia.contrib.language_models.mlx_lm import mlx_lm_model\n",
    "import concordia.prefabs.entity as entity_prefabs\n",
    "import concordia.prefabs.game_master as game_master_prefabs\n",
    "from concordia.prefabs.simulation import generic as simulation\n",
    "from concordia.typing import prefab as prefab_lib\n",
    "from concordia.utils import helper_functions\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "MLX-LM supports many quantized models from the `mlx-community` on HuggingFace.\n",
    "Popular choices include:\n",
    "\n",
    "| Model | Size | Memory | Notes |\n",
    "|-------|------|--------|-------|\n",
    "| `mlx-community/Llama-3.2-3B-Instruct-4bit` | 3B | ~2GB | Fast, good for testing |\n",
    "| `mlx-community/Llama-3.1-8B-Instruct-4bit` | 8B | ~5GB | Good balance |\n",
    "| `mlx-community/Mistral-7B-Instruct-v0.3-4bit` | 7B | ~4GB | Strong performance |\n",
    "| `mlx-community/Qwen2.5-7B-Instruct-4bit` | 7B | ~4GB | Excellent reasoning |\n",
    "| `mlx-community/gemma-2-9b-it-4bit` | 9B | ~6GB | Google's latest |\n",
    "\n",
    "The model will be downloaded automatically on first use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Model Configuration\n",
    "\n",
    "# Choose a model from mlx-community on HuggingFace\n",
    "# Smaller models are faster but less capable\n",
    "MODEL_NAME = 'mlx-community/Llama-3.2-3B-Instruct-4bit'\n",
    "\n",
    "# Optional: Path to a LoRA adapter for fine-tuned behavior\n",
    "ADAPTER_PATH = None  # e.g., '/path/to/my/adapter'\n",
    "\n",
    "# System message for the model\n",
    "SYSTEM_MESSAGE = (\n",
    "    'You always continue sentences provided by the user and you never repeat '\n",
    "    'what the user already said.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initialize the MLX-LM model\n",
    "\n",
    "print(f'Loading model: {MODEL_NAME}')\n",
    "print('This may take a moment on first run as the model is downloaded...')\n",
    "\n",
    "model = mlx_lm_model.MLXLMLanguageModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    adapter_path=ADAPTER_PATH,\n",
    "    system_message=SYSTEM_MESSAGE,\n",
    ")\n",
    "\n",
    "print('Model loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup sentence encoder for memory retrieval\n",
    "\n",
    "st_model = sentence_transformers.SentenceTransformer(\n",
    "    'sentence-transformers/all-mpnet-base-v2'\n",
    ")\n",
    "embedder = lambda x: st_model.encode(x, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test the model with a simple prompt\n",
    "\n",
    "test_prompt = 'What makes a good leader?'\n",
    "print(f'Prompt: {test_prompt}\\n')\n",
    "\n",
    "response = model.sample_text(\n",
    "    test_prompt,\n",
    "    max_tokens=150,\n",
    "    temperature=0.7,\n",
    ")\n",
    "print(f'Response:\\n{response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test the choice selection\n",
    "\n",
    "prompt = 'When faced with a difficult decision, the wisest course of action is usually to'\n",
    "choices = [\n",
    "    'act quickly and decisively',\n",
    "    'gather more information first',\n",
    "    'seek advice from others',\n",
    "    'trust your intuition',\n",
    "]\n",
    "\n",
    "idx, choice, debug = model.sample_choice(prompt, choices)\n",
    "\n",
    "print(f'Prompt: {prompt}\\n')\n",
    "print(f'Selected: \"{choice}\" (index {idx})\\n')\n",
    "print('Log probabilities:')\n",
    "for c, logprob in debug['logprobs'].items():\n",
    "    print(f'  {c}: {logprob:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load prefabs\n",
    "\n",
    "prefabs = {\n",
    "    **helper_functions.get_package_classes(entity_prefabs),\n",
    "    **helper_functions.get_package_classes(game_master_prefabs),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Simulation: Two Philosophers Debating\n",
    "\n",
    "Let's run a simple simulation where two philosophers discuss the nature of knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configure the simulation\n",
    "\n",
    "instances = [\n",
    "    prefab_lib.InstanceConfig(\n",
    "        prefab='conversational__Entity',\n",
    "        role=prefab_lib.Role.ENTITY,\n",
    "        params={\n",
    "            'name': 'Socrates',\n",
    "            'conversation_style': (\n",
    "                'Speaks with careful reasoning, often using questions to '\n",
    "                'guide the conversation. Values wisdom and self-knowledge.'\n",
    "            ),\n",
    "        },\n",
    "    ),\n",
    "    prefab_lib.InstanceConfig(\n",
    "        prefab='conversational__Entity',\n",
    "        role=prefab_lib.Role.ENTITY,\n",
    "        params={\n",
    "            'name': 'Aristotle',\n",
    "            'conversation_style': (\n",
    "                'Speaks methodically, categorizing and analyzing concepts. '\n",
    "                'Appreciates empirical observation and practical wisdom.'\n",
    "            ),\n",
    "        },\n",
    "    ),\n",
    "    prefab_lib.InstanceConfig(\n",
    "        prefab='dialogic__GameMaster',\n",
    "        role=prefab_lib.Role.GAME_MASTER,\n",
    "        params={\n",
    "            'name': 'conversation rules',\n",
    "            'next_game_master_name': 'conversation rules',\n",
    "            'acting_order': 'fixed',\n",
    "            'can_terminate_simulation': False,\n",
    "        },\n",
    "    ),\n",
    "    prefab_lib.InstanceConfig(\n",
    "        prefab='formative_memories_initializer__GameMaster',\n",
    "        role=prefab_lib.Role.INITIALIZER,\n",
    "        params={\n",
    "            'name': 'initial setup rules',\n",
    "            'next_game_master_name': 'conversation rules',\n",
    "            'shared_memories': [\n",
    "                'Socrates and Aristotle are meeting in the agora to discuss philosophy.',\n",
    "                'The topic of their discussion is the nature of knowledge and virtue.',\n",
    "            ],\n",
    "            'player_specific_memories': {\n",
    "                'Socrates': [\n",
    "                    'Believes that true knowledge comes from within through questioning.',\n",
    "                    'Famous for saying \"I know that I know nothing.\"',\n",
    "                ],\n",
    "                'Aristotle': [\n",
    "                    'Was once a student at Plato\\'s Academy.',\n",
    "                    'Believes knowledge comes from observing the natural world.',\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "config = prefab_lib.Config(\n",
    "    default_premise=(\n",
    "        'Two great philosophers meet in ancient Athens to discuss the '\n",
    "        'fundamental nature of knowledge and how one can live a virtuous life.'\n",
    "    ),\n",
    "    default_max_steps=10,\n",
    "    prefabs=prefabs,\n",
    "    instances=instances,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initialize the simulation\n",
    "\n",
    "runnable_simulation = simulation.Simulation(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    embedder=embedder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run the simulation\n",
    "\n",
    "print('Running simulation...')\n",
    "print('(This may take a while depending on your model and hardware)\\n')\n",
    "\n",
    "raw_log = []\n",
    "results_log = runnable_simulation.play(\n",
    "    max_steps=6,\n",
    "    raw_log=raw_log,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Display the results\n",
    "\n",
    "display.HTML(results_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tips\n",
    "\n",
    "1. **Use quantized models**: 4-bit quantized models (ending in `-4bit`) use much less memory and run faster.\n",
    "\n",
    "2. **Match model size to your RAM**: \n",
    "   - 8GB RAM: Use 3B models\n",
    "   - 16GB RAM: Use 7-8B models  \n",
    "   - 32GB+ RAM: Can use larger models\n",
    "\n",
    "3. **Reduce max_tokens**: Shorter generations are faster.\n",
    "\n",
    "4. **Use lower temperatures**: Temperature=0 uses greedy decoding which is slightly faster.\n",
    "\n",
    "5. **Consider using LoRA adapters**: Fine-tune a small adapter instead of using a larger base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Copyright 2025 DeepMind Technologies Limited.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
